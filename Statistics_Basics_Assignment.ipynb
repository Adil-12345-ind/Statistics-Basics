{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Basics"
      ],
      "metadata": {
        "id": "PM5ZWkwov4il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is statistics, and why is it important?\n",
        " - Statistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It's a vital tool for understanding the world around us because it allows us to make sense of complex information, identify patterns, and make informed decisions in various fields.\n",
        "Here's why statistics is important:\n",
        "Understanding Data:\n",
        "Statistics provides methods to summarize and interpret data, making it easier to grasp large amounts of information.\n",
        "Informed Decision Making:\n",
        "By analyzing data, statistics helps in making better decisions in areas like business, healthcare, and policymaking.\n",
        "Identifying Trends and Patterns:\n",
        "Statistical analysis can reveal trends and patterns that might not be obvious from raw data, helping to predict future outcomes.\n",
        "Scientific Research:\n",
        "Statistics is fundamental to research across various disciplines, allowing scientists to draw conclusions from experiments and studies.\n",
        "Quality Control:\n",
        "In manufacturing and other industries, statistics is used to monitor and improve the quality of products and processes.\n",
        "Risk Assessment:\n",
        "Statistics plays a crucial role in assessing and managing risks in finance, insurance, and other fields.\n",
        "Social Sciences:\n",
        "Statistics is essential for understanding social phenomena, conducting surveys, and analyzing social trends.\n",
        "Everyday Life:\n",
        "Statistics is used in everyday situations, from understanding weather forecasts to making informed consumer choices.\n",
        "Communication:\n",
        "Statistical data can be presented in a clear and understandable way, making it easier to communicate complex information to others."
      ],
      "metadata": {
        "id": "2s8YJfD-wAM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the two main types of statistics?\n",
        " - The two main types of statistics are Descriptive Statistics and Inferential Statistics. Descriptive statistics focuses on summarizing and describing the basic features of a dataset, while inferential statistics uses sample data to make inferences and predictions about a larger population.\n",
        "Descriptive Statistics:\n",
        "Purpose:\n",
        "To summarize and describe the main characteristics of a dataset, such as its central tendency, spread, and shape.\n",
        "Examples:\n",
        "Calculating the mean, median, and mode of a dataset; creating histograms, bar charts, and scatter plots to visualize data; computing standard deviation and variance to assess variability.\n",
        "Focus:\n",
        "Presenting the data as it is, without making assumptions or drawing conclusions beyond the immediate dataset.\n",
        "Inferential Statistics:\n",
        "Purpose:\n",
        "To make inferences and predictions about a larger population based on data collected from a sample of that population.\n",
        "Examples:\n",
        "Conducting hypothesis tests to determine if there's a statistically significant difference between two groups; constructing confidence intervals to estimate the range within which the true population parameter is likely to fall; using regression analysis to predict the value of one variable based on the value of another.\n",
        "Focus:\n",
        "Drawing conclusions about the population based on sample data, understanding that there may be some uncertainty or variability in these inferences."
      ],
      "metadata": {
        "id": "vg5ki_rlwzWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are descriptive statistics?\n",
        " - Descriptive statistics are used to summarize, organize, and describe the characteristics of a dataset in a meaningful way, without making inferences about a larger population. They focus on presenting the main features of the data through measures of central tendency, variability, and distribution shape.\n",
        "Key aspects of descriptive statistics:\n",
        "Summarization:\n",
        "Descriptive statistics condense large datasets into simpler, more manageable forms, such as tables, charts, and summary statistics.\n",
        "Description:\n",
        "They describe the characteristics of the data, including measures of central tendency (mean, median, mode), measures of variability (range, variance, standard deviation), and shape of the distribution (skewness, kurtosis).\n",
        "Purpose:\n",
        "The primary goal is to provide a clear and concise summary of the data, enabling researchers to understand patterns, trends, and distributions within the dataset.\n",
        "No Inference:\n",
        "Unlike inferential statistics, descriptive statistics do not draw conclusions or make generalizations about a larger population based on the sample data.\n",
        "Examples of descriptive statistics:\n",
        "Measures of Central Tendency:\n",
        "Mean: The average of a dataset.\n",
        "Median: The middle value in a sorted dataset.\n",
        "Mode: The most frequent value in a dataset.\n",
        "Measures of Variability:\n",
        "Range: The difference between the highest and lowest values.\n",
        "Variance: A measure of how spread out the data is.\n",
        "Standard Deviation: The square root of the variance, also indicating spread.\n",
        "Other Measures:\n",
        "Frequency: How often a particular value occurs.\n",
        "Percentiles and Quartiles: Dividing the data into sections to show position.\n",
        "Graphical Representations: Histograms, bar charts, pie charts, scatter plots, etc., are used to visualize the data."
      ],
      "metadata": {
        "id": "YepDhh86SS6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is inferential statistics?\n",
        " - Inferential statistics is a branch of statistics that uses data from a sample to make generalizations or predictions about a larger population. It allows researchers to draw conclusions and make inferences about a population based on information gathered from a smaller, representative subset. Essentially, it moves beyond describing the sample data to making broader statements about the whole group.\n",
        "Here's a more detailed explanation:\n",
        "Key Concepts:\n",
        "Population: The entire group of individuals, objects, or events that a researcher is interested in studying.\n",
        "Sample: A subset of the population that is selected for analysis.\n",
        "Inference: A conclusion or generalization about the population based on the sample data.\n",
        "Hypothesis Testing: A common inferential technique used to determine whether there is enough evidence to support a claim about the population.\n",
        "Confidence Intervals: A range of values that is likely to contain the true population parameter.\n",
        "Generalization: Applying the findings from the sample to the entire population.\n",
        "How it Works:\n",
        "Sampling: Researchers collect data from a representative sample of the population.\n",
        "Analysis: They use statistical methods to analyze the sample data, such as calculating means, standard deviations, or conducting hypothesis tests.\n",
        "Inference: Based on the sample analysis, they make inferences or predictions about the larger population.\n",
        "Examples:\n",
        "A medical researcher might use inferential statistics to determine if a new drug is effective for a larger group of patients based on the results of a clinical trial with a smaller group of participants.\n",
        "A political pollster might use inferential statistics to estimate the percentage of voters who support a particular candidate based on a survey of a random sample of voters.\n",
        "A social scientist might use inferential statistics to study the relationship between income and education level based on a sample of individuals from a specific region."
      ],
      "metadata": {
        "id": "yrdyofPGSmPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is sampling in statistics?\n",
        " - In statistics, sampling is the process of selecting a subset of individuals or data points from a larger population to study and draw conclusions about the entire population. It's used when it's impractical or impossible to analyze the whole population due to size, cost, or time constraints.\n",
        "Here's a more detailed explanation:\n",
        "Why sample?\n",
        "Feasibility:\n",
        "Analyzing the entire population is often too expensive, time-consuming, or physically impossible.\n",
        "Efficiency:\n",
        "Sampling allows researchers to gather data and make inferences about the population more quickly and cost-effectively.\n",
        "Destructive testing:\n",
        "Sometimes, testing a sample destroys the item being tested, making it impossible to test the entire population.\n",
        "Inferences:\n",
        "Well-chosen samples can accurately represent the characteristics of the larger population, allowing researchers to make informed decisions and predictions.\n",
        "Key concepts in sampling:\n",
        "Population:\n",
        "The entire group of individuals or items that a researcher is interested in.\n",
        "Sample:\n",
        "A subset of the population that is selected for analysis.\n",
        "Sampling frame:\n",
        "A list of all the individuals or items in the population from which the sample will be drawn.\n",
        "Sampling methods:\n",
        "Different techniques used to select a sample from the population, such as random sampling, stratified sampling, or cluster sampling.\n",
        "Sample statistic:\n",
        "A numerical value calculated from the sample data (e.g., sample mean, sample standard deviation).\n",
        "Sampling error:\n",
        "The difference between the sample statistic and the true population parameter.\n",
        "Types of sampling methods:\n",
        "Probability sampling:\n",
        "Uses random selection, ensuring each member of the population has a known chance of being selected. This allows for statistical inference and minimizes bias.\n",
        "Non-probability sampling:\n",
        "Relies on non-random selection methods, often based on convenience or specific criteria. While easier to implement, it may introduce bias and limit generalizability."
      ],
      "metadata": {
        "id": "KCrvRM1YS3o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are the different types of sampling methods?\n",
        " - Sampling methods can be broadly categorized into probability (or random) sampling and non-probability (or non-random) sampling. Within each category, there are several specific techniques.\n",
        "Probability Sampling Methods:\n",
        "\n",
        "Simple Random Sampling:\n",
        "Every member of the population has an equal and random chance of being selected.\n",
        "\n",
        "Stratified Sampling:\n",
        "The population is divided into subgroups (strata), and then random samples are drawn from each stratum.\n",
        "\n",
        "Cluster Sampling:\n",
        "The population is divided into clusters, and then some clusters are randomly selected, with all individuals within those clusters being included in the sample.\n",
        "Systematic Sampling:\n",
        "A random starting point is selected, and then every k-th element is chosen for the sample.\n",
        "Non-Probability Sampling Methods:\n",
        "\n",
        "Convenience Sampling:\n",
        "Individuals are selected based on their ease of accessibility.\n",
        "\n",
        "Quota Sampling:\n",
        "The population is divided into subgroups, and then a predetermined number of individuals (a quota) are selected from each subgroup.\n",
        "Purposive Sampling:\n",
        "Individuals are selected based on specific characteristics or criteria relevant to the research question.\n",
        "\n",
        "Snowball Sampling:\n",
        "Participants are asked to recommend other potential participants who fit the study criteria.\n",
        "Consecutive Sampling:\n",
        "All individuals who meet the criteria are included in the sample, one after another.\n",
        "Voluntary Sampling:\n",
        "Individuals self-select to participate in the sample."
      ],
      "metadata": {
        "id": "7g6-edYZTJUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the difference between random and non-random sampling?\n",
        " - Random and non-random sampling are two main approaches to selecting a sample from a population for research or analysis. The key difference lies in the selection process: random sampling uses chance to ensure each member has an equal opportunity of being selected, while non-random sampling uses criteria like convenience, judgment, or specific researcher choices.\n",
        "Random Sampling:\n",
        "Definition:\n",
        "Random sampling involves selecting individuals from a population in such a way that each member has a known and equal probability of being chosen.\n",
        "Example:\n",
        "Drawing names from a hat, or using a random number generator to select participants.\n",
        "Benefits:\n",
        "Minimizes bias, provides a representative sample, and allows for statistical inference about the population.\n",
        "Methods:\n",
        "Includes techniques like simple random sampling, stratified random sampling, cluster sampling, and systematic sampling.\n",
        "Limitations:\n",
        "Can be time-consuming and expensive, and may not always be feasible, especially if the population is difficult to define or access.\n",
        "Non-Random Sampling:\n",
        "Definition:\n",
        "Non-random sampling involves selecting individuals based on specific criteria, convenience, or the researcher's judgment.\n",
        "Example:\n",
        "Conducting a survey at a mall, interviewing people who are available, or selecting experts in a particular field.\n",
        "Benefits:\n",
        "Often faster and cheaper than random sampling, can be useful for exploratory studies, and may be necessary when access to a full population is limited.\n",
        "Methods:\n",
        "Includes techniques like convenience sampling, purposive sampling, quota sampling, and snowball sampling.\n",
        "Limitations:\n",
        "May introduce bias, can't generalize findings to the larger population, and may not be suitable for conclusive research."
      ],
      "metadata": {
        "id": "dxVfFKxpTXiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Define and give examples of qualitative and quantitative data.\n",
        " - Qualitative data describes qualities or characteristics that can't be measured numerically, while quantitative data describes quantities that can be measured numerically.\n",
        "Qualitative Data:\n",
        "Definition: Information that is not expressed numerically and is often used to understand the qualities, characteristics, or meanings behind something.\n",
        "Examples:\n",
        "The color of a flower (red, blue, yellow).\n",
        "The taste of an apple (sweet, tart, acidic).\n",
        "A person's opinion on a product (positive, negative, neutral).\n",
        "Ethnographic observations (describing cultural practices).\n",
        "Data Collection Methods: Interviews, focus groups, observations, document analysis.\n",
        "Quantitative Data:\n",
        "Definition: Information that can be expressed numerically and is used to measure or count things.\n",
        "Examples:\n",
        "Height (in inches or centimeters).\n",
        "Weight (in pounds or kilograms).\n",
        "Age (in years).\n",
        "The number of students in a class.\n",
        "Data Collection Methods: Surveys, experiments, statistical data collection."
      ],
      "metadata": {
        "id": "b4qUe8LPTquz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the different types of data in statistics?\n",
        " - In statistics, data is broadly classified into qualitative (categorical) and quantitative (numerical) data. Qualitative data describes qualities or attributes, while quantitative data represents quantities or can be measured numerically. Both types have further subdivisions: qualitative into nominal and ordinal, and quantitative into discrete and continuous.\n",
        "Qualitative (Categorical) Data:\n",
        "Nominal:\n",
        "Categories without a natural order or ranking (e.g., colors, genders, types of fruits).\n",
        "Ordinal:\n",
        "Categories with a meaningful order or ranking, but the differences between categories may not be equal (e.g., survey responses on a Likert scale, rankings in a sports competition).\n",
        "Quantitative (Numerical) Data:\n",
        "Discrete:\n",
        "Data that can only take on specific, separate values, often whole numbers (e.g., number of cars, number of students, number of occurrences).\n",
        "Continuous:\n",
        "Data that can take any value within a given range (e.g., height, weight, temperature).\n",
        "Further Considerations:\n",
        "Interval and Ratio:\n",
        "Quantitative data can also be further categorized as interval (differences between values are meaningful, but there's no true zero point, e.g., temperature in Celsius) and ratio (differences between values are meaningful and there's a true zero point, e.g., height, weight).\n",
        "Level of Measurement:\n",
        "The type of data influences the appropriate statistical analysis techniques. For example, nominal data is often analyzed using frequencies and proportions, while continuous data might be analyzed using averages, standard deviations, and regression."
      ],
      "metadata": {
        "id": "RD50mDipT_Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain nominal, ordinal, interval, and ratio levels of measurement\n",
        " - The four levels of measurement, from least to most sophisticated, are nominal, ordinal, interval, and ratio. Each level builds upon the previous one, adding more information about the data.\n",
        "\n",
        "Nominal Level:\n",
        "Definition:\n",
        "This is the most basic level, where data is categorized into mutually exclusive and non-overlapping categories.\n",
        "Characteristics:\n",
        "No inherent order or ranking among categories, and no mathematical operations can be performed.\n",
        "Examples:\n",
        "Colors (red, blue, green), types of fruit (apple, banana, orange), or gender (male, female).\n",
        "\n",
        "Ordinal Level:\n",
        "Definition: Data can be categorized and ranked in a meaningful order.\n",
        "Characteristics: While order is established, the exact differences between values may not be quantifiable or equal.\n",
        "Examples: Education level (high school, bachelor's, master's), customer satisfaction (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied), or rankings in a competition (1st, 2nd, 3rd)."
      ],
      "metadata": {
        "id": "-pi55FJYVDOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the measure of central tendency?\n",
        " - Measures of central tendency are statistics that represent the central or typical value of a dataset. They help summarize data by providing a single value that represents the \"middle\" of the data distribution. The three main measures are:\n",
        "Mean: The average of all data points.\n",
        "Median: The middle value when the data is ordered.\n",
        "Mode: The most frequently occurring value in the dataset."
      ],
      "metadata": {
        "id": "x8ijk9z9VelB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Define mean, median, and mode.\n",
        " - Mean, median, and mode are measures of central tendency that describe the \"average\" or typical value in a dataset. The mean is the average, calculated by summing all values and dividing by the number of values. The median is the middle value when the data is sorted. The mode is the value that appears most frequently.\n",
        "Here's a more detailed explanation:\n",
        "Mean:\n",
        "To find the mean, add up all the numbers in a dataset and then divide by the total count of numbers. For example, the mean of 2, 4, and 6 is (2+4+6)/3 = 4.\n",
        "Median:\n",
        "To find the median, first arrange the numbers in ascending order. The median is the middle number. If there's an even number of values, the median is the average of the two middle numbers. For example, the median of 2, 4, and 6 is 4.\n",
        "Mode:\n",
        "The mode is the number that appears most often in a dataset. For example, in the set {2, 4, 4, 6}, the mode is 4.\n",
        "In summary:\n",
        "Mean: Sum of all values divided by the number of values.\n",
        "Median: Middle value when data is sorted.\n",
        "Mode: Most frequently occurring value."
      ],
      "metadata": {
        "id": "0i5ctD9iV0kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the significance of the measure of central tendency?\n",
        " -\n",
        "Measures of central tendency, like mean, median, and mode, are crucial in statistics because they provide a single, representative value to describe a dataset. They help condense large datasets into a more manageable form, facilitating understanding and comparison of different datasets.\n",
        "Significance of Central Tendency:\n",
        "Summarization: They provide a concise overview of a dataset's central location.\n",
        "Comparison: They allow for easy comparison of different datasets by representing their \"typical\" values.\n",
        "Decision-Making: In various fields, including business and research, these measures help in making informed decisions based on data analysis.\n",
        "Data Description: They help describe the general nature of a dataset's distribution.\n",
        "Foundation for Further Analysis: They are the basis for more advanced statistical analyses."
      ],
      "metadata": {
        "id": "Y5IQ-XG0WC6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is variance, and how is it calculated?\n",
        " - Variance is a statistical measure that quantifies the spread or dispersion of data points in a dataset. It indicates how much the individual values in a dataset deviate from the mean (average) value. A higher variance means the data points are more spread out, while a lower variance indicates they are clustered closer to the mean.\n",
        "Calculation:\n",
        "Calculate the mean: Sum all the values in the dataset and divide by the number of values.\n",
        "Find the difference from the mean: Subtract the mean from each individual value in the dataset. [2-5=-3, 5-5=0, 8-5=3]\n",
        "Square the differences: Square each of the differences calculated in the previous step. [(-3)^2=9, 0^2=0, 3^2=9]\n",
        "Sum the squared differences: Add up all the squared differences. [9+0+9=18]\n",
        "Divide by (n-1): For a sample variance, divide the sum of squared differences by the number of values minus one (n-1). For population variance, divide by the total number of values (N). [18/ (3-1) =\n",
        "Formula:\n",
        "Sample Variance (sÂ²):\n",
        "sÂ² = Î£ (xi - xÌ„)Â² / (n - 1), where xi is each data point, xÌ„ is the sample mean, and n is the number of data points in the sample.\n",
        "Population Variance (ÏƒÂ²):\n",
        "ÏƒÂ² = Î£ (xi - Î¼)Â² / N, where xi is each data point, Î¼ is the population mean, and N is the number of data points in the population."
      ],
      "metadata": {
        "id": "XKGgjYr4WfJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What is standard deviation, and why is it important?\n",
        " - Standard deviation is a statistical measure that quantifies the amount of variation or dispersion of a set of data values around its mean (average). A low standard deviation indicates that the data points tend to be very close to the mean, while a high standard deviation indicates that the data points are spread out over a wider range of values. It's crucial for understanding data reliability and risk, especially in fields like finance and quality control.\n",
        "Here's a more detailed explanation:\n",
        "What it measures:\n",
        "Standard deviation tells you how much the individual values in a dataset differ from the average.\n",
        "Low standard deviation:\n",
        "Suggests that the data points are clustered tightly around the mean, indicating less variability.\n",
        "High standard deviation:\n",
        "Indicates that the data points are more spread out, showing greater variability.\n",
        "Why it's important:\n",
        "Data reliability: A low standard deviation suggests that the mean is a more reliable representation of the data because most values are close to it.\n",
        "Risk assessment: In finance, a higher standard deviation for investment returns indicates greater risk, meaning the actual returns could deviate significantly from the average.\n",
        "Quality control: In manufacturing, a low standard deviation for product dimensions indicates more consistent quality.\n",
        "Understanding distributions: Standard deviation helps understand the spread of data in relation to the mean, especially in datasets that follow a normal distribution (bell curve). For example, in a normal distribution, roughly 68% of the data falls within one standard deviation of the mean, and about 95% falls within two standard deviations.\n",
        "Comparing datasets: Standard deviation allows for the comparison of different datasets with varying spreads.\n",
        "How it's used:\n",
        "Standard deviation is widely used in various fields, including:\n",
        "Finance: To assess investment risk and volatility.\n",
        "Quality control: To monitor the consistency of products and processes.\n",
        "Research: To analyze data variability and draw meaningful conclusions.\n",
        "Healthcare: To understand patient outcomes and the effectiveness of treatments."
      ],
      "metadata": {
        "id": "mS-p2MAxXNuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Define and explain the term range in statistics.\n",
        " - In statistics, the range is a simple measure of variability that represents the difference between the highest and lowest values in a dataset. It provides a quick overview of the spread or dispersion of the data.\n",
        "Definition and Calculation:\n",
        "Definition: The range is the difference between the maximum and minimum values within a dataset.\n",
        "Calculation: Range = Highest Value - Lowest Value.\n",
        "Example:\n",
        "If a dataset contains the values {2, 4, 6, 8, 12}, the range would be calculated as:"
      ],
      "metadata": {
        "id": "1mhij9JUXkM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Range = 12 - 2 = 10"
      ],
      "metadata": {
        "id": "3_XkyHwRYR7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significance:\n",
        "Spread of data: The range provides a general idea of how spread out the data points are.\n",
        "Easy to calculate: It is one of the simplest measures of variability to calculate.\n",
        "Limitations: The range is sensitive to outliers (extreme values) and can be misleading if the dataset has a small number of values.\n"
      ],
      "metadata": {
        "id": "LmF4MHSOYU4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the difference between variance and standard deviation?\n",
        " - Variance and standard deviation are both measures of data dispersion, but standard deviation is the square root of the variance. Variance quantifies the average squared difference of data points from the mean, while standard deviation represents the spread of data around the mean, expressed in the same units as the original data.\n",
        "Here's a more detailed breakdown:\n",
        "Variance:\n",
        "Measures the average degree to which each point in a data set differs from the mean.\n",
        "Calculated by squaring the differences between each data point and the mean, summing these squared differences, and then dividing by the number of data points (or number of data points minus one for a sample).\n",
        "Results in a value expressed in squared units, which can make it difficult to interpret directly.\n",
        "Standard Deviation:\n",
        "The square root of the variance.\n",
        "Represents the typical distance of data points from the mean.\n",
        "Expressed in the same units as the original data, making it more intuitive to understand the spread of the data.\n",
        "In essence:\n",
        "Variance gives you a sense of the overall variability in a dataset, while standard deviation provides a more directly interpretable measure of how spread out the data is.\n",
        "Standard deviation is often preferred for its interpretability, but variance is a crucial step in its calculation and is also used in statistical tests."
      ],
      "metadata": {
        "id": "kq-aL0JHYZLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is skewness in a dataset?\n",
        " - Skewness in a dataset refers to the measure of its asymmetry, or the lack of symmetry, in its distribution. It indicates whether the data is more concentrated on one side of the mean compared to the other. A symmetric distribution, like a normal distribution, has a balanced, bell-shaped curve with the mean, median, and mode all equal. In contrast, skewed distributions have a longer tail on one side, either the left or the right, indicating that extreme values are present on that side.\n",
        "Types of Skewness:\n",
        "Positive Skewness (Right Skew):\n",
        "The tail of the distribution is longer on the right side, with more extreme values on the higher end. In this case, the mean is greater than the median.\n",
        "Negative Skewness (Left Skew):\n",
        "The tail of the distribution is longer on the left side, with more extreme values on the lower end. Here, the mean is less than the median.\n",
        "Zero Skewness (Symmetrical):\n",
        "The distribution is symmetrical, with the mean, median, and mode being equal.\n",
        "Understanding Skewness:\n",
        "Direction of Outliers:\n",
        "Skewness helps determine the direction of outliers, indicating whether they tend to be higher or lower values.\n",
        "Impact on Descriptive Statistics:\n",
        "Skewness can affect the mean, median, and mode, making them unequal in skewed distributions.\n",
        "Choice of Statistical Tests:\n",
        "The presence of skewness might require the use of non-parametric statistical tests, as many tests assume a normal distribution.\n",
        "Data Transformation:\n",
        "Skewness can be addressed through data transformations, such as taking the square root, cube root, logarithm, or reciprocal of the data points to make the distribution more symmetrical."
      ],
      "metadata": {
        "id": "CVrNdRqhY4g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does it mean if a dataset is positively or negatively skewed?\n",
        " - In statistics, skewness describes the asymmetry of a distribution. A dataset is positively skewed (or skewed right) when the tail of the distribution is longer on the right side, meaning there are more low values and a few very high values. Conversely, a dataset is negatively skewed (or skewed left) when the tail is longer on the left side, indicating more high values and a few very low values.\n",
        "Here's a more detailed explanation:\n",
        "Positive Skew (Right Skew):\n",
        "The majority of the data points are clustered towards the lower end of the scale.\n",
        "There are a few extremely high values that pull the mean (average) to the right, making it greater than the median (the middle value).\n",
        "Think of income distribution: most people earn in the lower to middle range, with a few high earners skewing the average.\n",
        "Negative Skew (Left Skew):\n",
        "The majority of the data points are clustered towards the higher end of the scale.\n",
        "There are a few extremely low values that pull the mean to the left, making it smaller than the median.\n",
        "An example might be the distribution of exam scores where most students did well, but a few scored very low.\n",
        "In simpler terms:\n",
        "Imagine a graph of your data. If the \"tail\" of the graph (the part that tapers off) is on the right, it's positively skewed. If the tail is on the left, it's negatively skewed.\n",
        "Positive skew means the average is higher than the middle, and negative skew means the average is lower than the middle."
      ],
      "metadata": {
        "id": "mz56ZJd0ZNCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Define and explain kurtosis.\n",
        " - Kurtosis is a statistical measure that describes the shape of a probability distribution, specifically the \"peakedness\" or \"tailedness\" of the distribution relative to a normal distribution. It indicates how much the data is concentrated around the mean (peak) and in the tails of the distribution.\n",
        "Here's a breakdown:\n",
        "Peakedness/Tailedness:\n",
        "Kurtosis essentially tells you whether a distribution has a sharp peak and heavy tails (leptokurtic), a flat peak and light tails (platykurtic), or something in between (mesokurtic).\n",
        "Normal Distribution:\n",
        "A normal distribution has a kurtosis of 3 (or an excess kurtosis of 0) and is considered mesokurtic.\n",
        "Excess Kurtosis:\n",
        "Often, statisticians calculate excess kurtosis, which is the kurtosis value minus 3. This makes it easier to interpret the kurtosis relative to a normal distribution.\n",
        "Leptokurtic (Excess Kurtosis > 0):\n",
        "Distributions with high kurtosis have a sharper peak and heavier tails, indicating more extreme values or outliers.\n",
        "Platykurtic (Excess Kurtosis < 0):\n",
        "Distributions with low kurtosis have a flatter peak and lighter tails, indicating fewer extreme values.\n",
        "Mesokurtic (Excess Kurtosis = 0):\n",
        "These distributions have a shape similar to a normal distribution.\n",
        "In simpler terms: Imagine plotting your data on a graph. Kurtosis helps you understand how much the data clusters near the center versus how often it deviates to extreme values in the tails.\n",
        "Examples:\n",
        "Finance:\n",
        "Kurtosis is used to assess risk. High kurtosis in stock returns might indicate a higher probability of large price swings.\n",
        "Quality Control:\n",
        "Kurtosis can help determine if product measurements are consistently within acceptable limits.\n",
        "Other Fields:\n",
        "Kurtosis is also used in various fields like risk management, quality control, and even in analyzing signals."
      ],
      "metadata": {
        "id": "lOGNZsNsZiYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is the purpose of covariance?\n",
        " - The covariance equation is used to determine the direction of the relationship between two variablesâ€”in other words, whether they tend to move in the same or opposite directions."
      ],
      "metadata": {
        "id": "f4OEwm0SZxqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What does correlation measure in statistics?\n",
        " - In statistics, correlation measures the strength and direction of a relationship between two or more variables. It indicates how closely the variables change together, either in the same direction (positive correlation) or in opposite directions (negative correlation). Importantly, correlation does not imply causation; just because two variables are correlated does not mean one causes the other.\n",
        "Here's a more detailed explanation:\n",
        "Strength of the relationship:\n",
        "Correlation quantifies how closely the data points cluster around a line of best fit. A stronger correlation means the data points are closer to the line, indicating a more predictable relationship.\n",
        "Direction of the relationship:\n",
        "Positive correlation: As one variable increases, the other tends to increase as well.\n",
        "Negative correlation: As one variable increases, the other tends to decrease.\n",
        "No correlation: There's no discernible relationship between the variables.\n",
        "Correlation coefficient:\n",
        "The correlation coefficient (often denoted as r) is a numerical value between -1 and +1 that represents the strength and direction of the correlation.\n",
        "A value of +1 indicates a perfect positive correlation.\n",
        "A value of -1 indicates a perfect negative correlation.\n",
        "A value of 0 indicates no linear correlation.\n",
        "Correlation vs. Causation:\n",
        "A common saying in statistics is \"correlation does not imply causation.\" This means that even if two variables are strongly correlated, it doesn't necessarily mean that one causes the other. There might be other factors influencing both variables, or the relationship might be coincidental.\n",
        "For example, there might be a positive correlation between ice cream sales and crime rates. However, this doesn't mean that eating ice cream causes crime. A more likely explanation is that both are influenced by a third factor, like warm weather."
      ],
      "metadata": {
        "id": "7v3SQQHsaEkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is the difference between covariance and correlation?\n",
        "- Covariance and correlation both measure the relationship between two variables, but they differ in how they represent the strength and scale of that relationship. Covariance indicates the direction of the relationship (positive or negative), while correlation also indicates the strength of that relationship and is standardized to a range between -1 and +1.\n",
        "Here's a more detailed breakdown:\n",
        "Covariance:\n",
        "Definition: Measures the extent to which two variables change together. A positive covariance indicates that when one variable increases, the other tends to increase as well, and vice versa for negative covariance.\n",
        "Range: Can range from negative infinity to positive infinity (-âˆž to +âˆž).\n",
        "Limitations: Covariance is affected by the scale of the variables. If you change the units of measurement (e.g., from kilograms to grams), the covariance value will change.\n",
        "Correlation:\n",
        "Definition: A standardized measure of the linear relationship between two variables. It not only indicates the direction of the relationship (positive or negative) but also its strength.\n",
        "Range: Always falls between -1 and +1 (-1 to +1).\n",
        "Benefits: Correlation is easier to interpret than covariance because it is standardized. A correlation of +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
        "Scale-independent: Correlation is not affected by changes in the scale of the variables.\n",
        "In essence:\n",
        "Covariance tells you if two variables change together and in which direction (positive or negative).\n",
        "Correlation tells you how strongly two variables are related and in which direction.\n",
        "Example:\n",
        "Imagine you are measuring the height and weight of individuals. If you calculate the covariance, it will depend on the units used (e.g., centimeters and kilograms). However, if you calculate the correlation, it will be the same regardless of whether you use centimeters/kilograms or inches/pounds"
      ],
      "metadata": {
        "id": "cgd1dDwEaTq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. What are some real-world applications of statistics?\n",
        " - Statistics are used in a wide variety of fields to analyze data, identify trends, and make informed decisions. Some examples include:\n",
        "Business:\n",
        "Analyzing sales data, market trends, and customer behavior.\n",
        "Healthcare:\n",
        "Conducting clinical trials, analyzing patient data, and understanding disease patterns.\n",
        "Finance:\n",
        "Assessing investment risks, predicting market fluctuations, and developing financial models.\n",
        "Government:\n",
        "Analyzing census data, tracking crime rates, and evaluating the effectiveness of policies.\n",
        "Education:\n",
        "Evaluating student performance, determining the effectiveness of teaching methods, and tracking graduation rates.\n",
        "Sports:\n",
        "Analyzing player statistics, predicting game outcomes, and evaluating team performance.\n",
        "Environmental Science:\n",
        "Monitoring pollution levels, tracking climate change trends, and assessing the impact of natural disasters.\n",
        "In addition, statistics are used in many other fields, including:\n",
        "Quality Control:\n",
        "Monitoring manufacturing processes, identifying defects, and ensuring product quality.\n",
        "Market Research:\n",
        "Identifying consumer preferences, understanding market segments, and developing marketing strategies.\n",
        "Insurance:\n",
        "Assessing risk, determining premiums, and managing claims.\n",
        "Sociology and Psychology:\n",
        "Analyzing survey data, understanding social trends, and researching human behavior."
      ],
      "metadata": {
        "id": "I_dLAldXajVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "d20MU4nNauJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  How do you calculate the mean, median, and mode of a dataset?\n",
        " - To calculate the mean, median, and mode of a dataset:\n",
        "Mean: Add up all the numbers in the dataset and divide the sum by the total number of items in the dataset.\n",
        "Median: Arrange the dataset in order from smallest to largest. The median is the middle number in the ordered list. If there's an even number of items, the median is the average of the two middle numbers.\n",
        "Mode: The mode is the number that appears most frequently in the dataset.\n",
        "Example:\n",
        "Let's say your dataset is: 2, 5, 3, 5, 8, 1, 5\n",
        "Mean: (2 + 5 + 3 + 5 + 8 + 1 + 5) / 7 = 29 / 7 = 4.14\n",
        "Median: 1, 2, 3, 5, 5, 5, 8. The median is 5\n",
        "Mode: 5 appears three times, which is more than any other number, so the mode is 5."
      ],
      "metadata": {
        "id": "JcCETMBxaxu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to compute the variance and standard deviation of a dataset.\n",
        " - A Python program to compute the variance and standard deviation of a dataset can be implemented using the statistics module for basic calculations or the numpy library for more advanced and efficient computations, especially with large datasets.\n",
        "Using the statistics module:"
      ],
      "metadata": {
        "id": "Xu0mlcCgbBiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def calculate_stats_statistics(data):\n",
        "    \"\"\"\n",
        "    Calculates variance and standard deviation using the statistics module.\n",
        "\n",
        "    Args:\n",
        "        data (list): A list of numerical data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the variance and standard deviation.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        return None, None\n",
        "\n",
        "    variance = statistics.variance(data)  # Sample variance (n-1 denominator)\n",
        "    std_dev = statistics.stdev(data)     # Sample standard deviation (n-1 denominator)\n",
        "\n",
        "    return variance, std_dev\n",
        "\n",
        "# Example usage\n",
        "dataset = [4, 8, 6, 5, 3, 2, 8, 9, 2, 5]\n",
        "variance_val, std_dev_val = calculate_stats_statistics(dataset)\n",
        "\n",
        "if variance_val is not None:\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    print(f\"Variance (statistics module): {variance_val}\")\n",
        "    print(f\"Standard Deviation (statistics module): {std_dev_val}\")\n",
        "else:\n",
        "    print(\"Dataset is empty.\")"
      ],
      "metadata": {
        "id": "V1K89bgcbPCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the numpy library:"
      ],
      "metadata": {
        "id": "RxtALv60bT8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_stats_numpy(data):\n",
        "    \"\"\"\n",
        "    Calculates variance and standard deviation using the numpy library.\n",
        "\n",
        "    Args:\n",
        "        data (list or numpy.ndarray): A list or NumPy array of numerical data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the variance and standard deviation.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        return None, None\n",
        "\n",
        "    data_array = np.array(data)\n",
        "\n",
        "    variance = np.var(data_array, ddof=1)  # ddof=1 for sample variance (n-1 denominator)\n",
        "    std_dev = np.std(data_array, ddof=1)   # ddof=1 for sample standard deviation (n-1 denominator)\n",
        "\n",
        "    return variance, std_dev\n",
        "\n",
        "# Example usage\n",
        "dataset = [4, 8, 6, 5, 3, 2, 8, 9, 2, 5]\n",
        "variance_val, std_dev_val = calculate_stats_numpy(dataset)\n",
        "\n",
        "if variance_val is not None:\n",
        "    print(f\"\\nDataset: {dataset}\")\n",
        "    print(f\"Variance (numpy): {variance_val}\")\n",
        "    print(f\"Standard Deviation (numpy): {std_dev_val}\")\n",
        "else:\n",
        "    print(\"Dataset is empty.\")"
      ],
      "metadata": {
        "id": "VTeiqdpobUhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a dataset and classify it into nominal, ordinal, interval, and ratio types.\n",
        " - Sure! Here's a simple dataset followed by a classification of each variable into nominal, ordinal, interval, or ratio types of data.\n",
        "\n",
        "ðŸŽ¯ Sample Dataset: Student Performance\n",
        "Student ID\tName\tGender\tGrade Level\tMath Score\tTemperature (Â°C)\tRanking\tHeight (cm)\n",
        "001\tAlice\tFemale\tFreshman\t78\t22\t3rd\t160\n",
        "002\tBob\tMale\tSophomore\t85\t20\t1st\t172\n",
        "003\tCharlie\tMale\tJunior\t65\t19\t5th\t168\n",
        "004\tDiana\tFemale\tSenior\t92\t21\t2nd\t158\n",
        "005\tEthan\tMale\tFreshman\t70\t23\t4th\t180\n",
        "\n",
        "ðŸ§  Data Type Classification\n",
        "Variable\tType\tExplanation\n",
        "Student ID\tNominal\tUsed as a label/identifier; no meaningful order or calculation.\n",
        "Name\tNominal\tCategorical label; can't be ordered or measured.\n",
        "Gender\tNominal\tCategorical label (e.g., Male/Female); no order.\n",
        "Grade Level\tOrdinal\tHas a meaningful order (Freshman < Sophomore < Junior < Senior), but differences are not uniform.\n",
        "Math Score\tRatio\tHas a true zero, and differences/ratios are meaningful (e.g., 80 is twice 40).\n",
        "Temperature (Â°C)\tInterval\tCan have negative values, and intervals are meaningful, but there's no true zero.\n",
        "Ranking\tOrdinal\tIndicates order (1st, 2ndâ€¦), but not the magnitude of difference between ranks.\n",
        "Height (cm)\tRatio\tTrue zero exists; can compare using ratios."
      ],
      "metadata": {
        "id": "OLdnwJ7AbXPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement sampling techniques like random sampling and stratified sampling.\n",
        " - Great! Letâ€™s demonstrate how to implement sampling techniques using the dataset we discussed earlier. Iâ€™ll show both Random Sampling and Stratified Sampling using Python and pandas. If you'd like me to run the code or export the data, let me know.\n",
        "\n"
      ],
      "metadata": {
        "id": "SfnplQm8gzBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Original dataset\n",
        "data = {\n",
        "    'Student ID': ['001', '002', '003', '004', '005'],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Ethan'],\n",
        "    'Gender': ['Female', 'Male', 'Male', 'Female', 'Male'],\n",
        "    'Grade Level': ['Freshman', 'Sophomore', 'Junior', 'Senior', 'Freshman'],\n",
        "    'Math Score': [78, 85, 65, 92, 70],\n",
        "    'Temperature (Â°C)': [22, 20, 19, 21, 23],\n",
        "    'Ranking': ['3rd', '1st', '5th', '2nd', '4th'],\n",
        "    'Height (cm)': [160, 172, 168, 158, 180]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "zJ_M4BQch_Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Sampling\n",
        "Randomly select 3 students from the dataset."
      ],
      "metadata": {
        "id": "ZYkHtJOYijJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random sampling of 3 students\n",
        "random_sample = df.sample(n=3, random_state=42)\n",
        "print(random_sample)\n"
      ],
      "metadata": {
        "id": "5MWuMZ74irHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stratified Sampling (by Gender)\n",
        "Ensure the sample maintains the proportion of genders.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "VJ2Uqg2-itxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified sampling by 'Gender'\n",
        "stratified_sample = df.groupby('Gender', group_keys=False).apply(lambda x: x.sample(frac=0.5, random_state=1))\n",
        "print(stratified_sample)\n"
      ],
      "metadata": {
        "id": "t6dVNCCZjFhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output Explanation\n",
        "Random Sampling selects any 3 rows without considering subgroup representation.\n",
        "\n",
        "Stratified Sampling keeps the gender ratio intact (e.g., if 40% of the dataset is female, 40% of the sample will be female)."
      ],
      "metadata": {
        "id": "IvqXKooCjLGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python function to calculate the range of a dataset.\n",
        " - Sure! Here's a simple Python function to calculate the range of a dataset. The range is defined as the difference between the maximum and minimum values in the dataset."
      ],
      "metadata": {
        "id": "adL-NYzTjbO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_range(data):\n",
        "    \"\"\"\n",
        "    Calculate the range of a numeric dataset.\n",
        "\n",
        "    Parameters:\n",
        "        data (list or iterable): A list of numeric values.\n",
        "\n",
        "    Returns:\n",
        "        float: The range (max - min) of the dataset.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        raise ValueError(\"The dataset is empty.\")\n",
        "\n",
        "    return max(data) - min(data)\n"
      ],
      "metadata": {
        "id": "OAm9WaUqkEnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Usage"
      ],
      "metadata": {
        "id": "vzhcaR2qkHPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "math_scores = [78, 85, 65, 92, 70]\n",
        "\n",
        "# Calculate range\n",
        "range_value = calculate_range(math_scores)\n",
        "print(\"Range of Math Scores:\", range_value)\n"
      ],
      "metadata": {
        "id": "vqmPCndVkL9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "lIUFbmVBkSEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Range of Math Scores: 27\n"
      ],
      "metadata": {
        "id": "5qz0h54RkTJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a dataset and plot its histogram to visualize skewness.\n",
        " - Great! Let's create a dataset that shows skewness and plot a histogram using Python.\n",
        "\n",
        "I'll show how to:\n",
        "\n",
        "Generate a right-skewed (positively skewed) dataset.\n",
        "\n",
        "Create a histogram to visualize skewness.\n",
        "\n",
        "(Optional) Calculate skewness numerically.\n",
        "\n",
        "âœ… Step 1: Create a Skewed Dataset\n",
        "We'll use NumPy to generate a right-skewed distribution using the exponential distribution."
      ],
      "metadata": {
        "id": "-9z9VhPRkmLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Generate right-skewed data\n",
        "np.random.seed(0)\n",
        "data = np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Convert to DataFrame (optional)\n",
        "df = pd.DataFrame(data, columns=['Value'])\n"
      ],
      "metadata": {
        "id": "rvKKlx_vk4YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Plot Histogram to Visualize Skewness"
      ],
      "metadata": {
        "id": "UpOZfteRlKPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(df['Value'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram of Right-Skewed Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CCHsfopelMrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Calculate Skewness Value"
      ],
      "metadata": {
        "id": "lp7CLdKClO9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate skewness\n",
        "skewness = skew(df['Value'])\n",
        "print(f\"Skewness: {skewness:.2f}\")\n"
      ],
      "metadata": {
        "id": "j23SlmsUlRYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Output:"
      ],
      "metadata": {
        "id": "mXPB_sALleko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Skewness: ~2.0 (positive â†’ right-skewed)\n"
      ],
      "metadata": {
        "id": "Lwf7YruhlfQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Calculate skewness and kurtosis of a dataset using Python libraries.\n",
        " - Absolutely! Here's how you can calculate skewness and kurtosis of a dataset using Python libraries like scipy and pandas.\n",
        " Example Dataset\n",
        "Let's use a sample dataset of exam scores:"
      ],
      "metadata": {
        "id": "E-9MfNSwlhfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Sample dataset: Exam scores\n",
        "data = [78, 85, 65, 92, 70, 88, 95, 55, 72, 60]\n",
        "\n",
        "# Convert to a pandas Series (optional, but useful)\n",
        "series = pd.Series(data)\n"
      ],
      "metadata": {
        "id": "boVrFjSOl0J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“ˆ Calculate Skewness"
      ],
      "metadata": {
        "id": "foHjRAg8l9HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SciPy\n",
        "skew_value = skew(series)\n",
        "\n",
        "# Using Pandas\n",
        "skew_pandas = series.skew()\n",
        "\n",
        "print(f\"Skewness (SciPy): {skew_value:.2f}\")\n",
        "print(f\"Skewness (Pandas): {skew_pandas:.2f}\")\n"
      ],
      "metadata": {
        "id": "LN-rIxQHl_sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¯ Calculate Kurtosis"
      ],
      "metadata": {
        "id": "SzXnbirymM5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SciPy (Fisherâ€™s definition by default: normal distribution = 0)\n",
        "kurt_value = kurtosis(series)\n",
        "\n",
        "# Using Pandas (also Fisher's definition)\n",
        "kurt_pandas = series.kurt()\n",
        "\n",
        "print(f\"Kurtosis (SciPy): {kurt_value:.2f}\")\n",
        "print(f\"Kurtosis (Pandas): {kurt_pandas:.2f}\")\n"
      ],
      "metadata": {
        "id": "wVc_OtCPmO4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Interpretation:\n",
        "Skewness:\n",
        "\n",
        "> 0: Right/positive skew\n",
        "\n",
        "< 0: Left/negative skew\n",
        "\n",
        "= 0: Symmetric\n",
        "\n",
        "Kurtosis:\n",
        "\n",
        "> 0: Leptokurtic (peaked, heavy tails)\n",
        "\n",
        "< 0: Platykurtic (flat, light tails)\n",
        "\n",
        "= 0: Mesokurtic (normal distribution)\n",
        "\n"
      ],
      "metadata": {
        "id": "AxGuaZwdmRar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Generate a dataset and demonstrate positive and negative skewness.\n",
        " - Great! Let's generate two datasets:\n",
        "\n",
        "ðŸ“ˆ One with positive skewness (right-skewed)\n",
        "\n",
        "ðŸ“‰ One with negative skewness (left-skewed)\n",
        "\n",
        "Weâ€™ll also:\n",
        "\n",
        "Visualize them with histograms\n",
        "\n",
        "Calculate skewness numerically\n",
        "\n",
        "âœ… Step-by-Step Python Code\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import"
      ],
      "metadata": {
        "id": "DgHbl9OzmXFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate datasets\n",
        "positive_skew = np.random.exponential(scale=2.0, size=1000)     # Right-skewed\n",
        "negative_skew = -np.random.exponential(scale=2.0, size=1000) + 10  # Left-skewed\n",
        "\n",
        "# Convert to DataFrames\n",
        "df = pd.DataFrame({\n",
        "    'Positive Skew': positive_skew,\n",
        "    'Negative Skew': negative_skew\n",
        "})\n"
      ],
      "metadata": {
        "id": "nR2zkHqomtkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Plot Histograms"
      ],
      "metadata": {
        "id": "Bk55mb6ImwXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot both histograms\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Positive skew\n",
        "axes[0].hist(df['Positive Skew'], bins=30, color='lightgreen', edgecolor='black')\n",
        "axes[0].set_title('Histogram: Positive Skew')\n",
        "axes[0].set_xlabel('Value')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Negative skew\n",
        "axes[1].hist(df['Negative Skew'], bins=30, color='salmon', edgecolor='black')\n",
        "axes[1].set_title('Histogram: Negative Skew')\n",
        "axes[1].set_xlabel('Value')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xE416MNbmzAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§® Calculate Skewness"
      ],
      "metadata": {
        "id": "zrD-d0T7m1F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate skewness\n",
        "pos_skewness = skew(df['Positive Skew'])\n",
        "neg_skewness = skew(df['Negative Skew'])\n",
        "\n",
        "print(f\"Positive Skewness: {pos_skewness:.2f}\")  # Should be > 0\n",
        "print(f\"Negative Skewness: {neg_skewness:.2f}\")  # Should be < 0\n"
      ],
      "metadata": {
        "id": "YZmqBbkbm4Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Example Output (Approximate)"
      ],
      "metadata": {
        "id": "v3xwYNtQm6f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Positive Skewness: 2.03\n",
        "Negative Skewness: -2.03\n"
      ],
      "metadata": {
        "id": "CgTZGi89m9c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python script to calculate covariance between two datasets.\n",
        " - Sure! Here's a complete Python script that calculates the covariance between two datasets using both manual computation and built-in functions.\n",
        "\n",
        "âœ… Python Script to Calculate Covariance"
      ],
      "metadata": {
        "id": "Xxxie48WnAQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample datasets\n",
        "X = [65, 66, 67, 68, 69]\n",
        "Y = [67, 68, 65, 70, 72]\n",
        "\n",
        "# Manual covariance calculation\n",
        "def calculate_covariance(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError(\"Datasets must be of the same length.\")\n",
        "\n",
        "    n = len(x)\n",
        "    mean_x = np.mean(x)\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    covariance = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)) / (n - 1)\n",
        "    return covariance\n",
        "\n",
        "# Using NumPy\n",
        "def numpy_covariance(x, y):\n",
        "    cov_matrix = np.cov(x, y)\n",
        "    return cov_matrix[0, 1]\n",
        "\n",
        "# Using Pandas\n",
        "df = pd.DataFrame({'X': X, 'Y': Y})\n",
        "pandas_covariance = df.cov().iloc[0, 1]\n",
        "\n",
        "# Print results\n",
        "print(\"Manual Covariance:\", calculate_covariance(X, Y))\n",
        "print(\"NumPy Covariance:\", numpy_covariance(X, Y))\n",
        "print(\"Pandas Covariance:\", pandas_covariance)\n"
      ],
      "metadata": {
        "id": "8kJEq9KDoAaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ Output (Approximate)\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "NWgmdsAUoEu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Manual Covariance: 2.5\n",
        "NumPy Covariance: 2.5\n",
        "Pandas Covariance: 2.5\n"
      ],
      "metadata": {
        "id": "DCiw0bptoFnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Summary\n",
        "Positive covariance â†’ variables increase together\n",
        "\n",
        "Negative covariance â†’ one increases as the other decreases\n",
        "\n",
        "Zero covariance â†’ no linear relationship"
      ],
      "metadata": {
        "id": "3t_sF6tFooEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Write a Python script to calculate the correlation coefficient between two datasets.\n",
        " - Sure! Here's a Python script that calculates the correlation coefficient (specifically, the Pearson correlation coefficient) between two datasets using:\n",
        "\n",
        "Manual formula\n",
        "\n",
        "NumPy\n",
        "\n",
        "Pandas\n",
        "\n",
        "SciPy\n",
        "\n",
        "âœ… Python Script: Correlation Coefficient"
      ],
      "metadata": {
        "id": "cQfHeM4NopFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Sample datasets\n",
        "X = [65, 66, 67, 68, 69]\n",
        "Y = [67, 68, 65, 70, 72]\n",
        "\n",
        "# Manual calculation of Pearson correlation\n",
        "def calculate_pearson_correlation(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError(\"Datasets must be the same length.\")\n",
        "\n",
        "    n = len(x)\n",
        "    mean_x = np.mean(x)\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))\n",
        "    denominator = (sum((x[i] - mean_x)**2 for i in range(n)) * sum((y[i] - mean_y)**2 for i in range(n))) ** 0.5\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "# Using NumPy\n",
        "numpy_corr = np.corrcoef(X, Y)[0, 1]\n",
        "\n",
        "# Using Pandas\n",
        "df = pd.DataFrame({'X': X, 'Y': Y})\n",
        "pandas_corr = df['X'].corr(df['Y'])\n",
        "\n",
        "# Using SciPy\n",
        "scipy_corr, _ = pearsonr(X, Y)\n",
        "\n",
        "# Display results\n",
        "print(\"Manual Pearson Correlation:\", round(calculate_pearson_correlation(X, Y), 3))\n",
        "print(\"NumPy Correlation:\", round(numpy_corr, 3))\n",
        "print(\"Pandas Correlation:\", round(pandas_corr, 3))\n",
        "print(\"SciPy Correlation:\", round(scipy_corr, 3))\n"
      ],
      "metadata": {
        "id": "ifEORPYso8n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Interpretation\n",
        "+1.0: Perfect positive linear relationship\n",
        "\n",
        "0: No linear relationship\n",
        "\n",
        "-1.0: Perfect negative linear relationship"
      ],
      "metadata": {
        "id": "dQXO9YLBpk6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Create a scatter plot to visualize the relationship between two variables.\n",
        " - ðŸ“Š How we can plot it:\n",
        "Data Source\n",
        "A ready-to-use dataset (â€œLife Expectancy vsÂ GDP per capita 1950â€“2018â€) is available on GitHub\n",
        "ourworldindata.org\n",
        "+3\n",
        "github.com\n",
        "+3\n",
        "medium.com\n",
        "+3\n",
        ".\n",
        "\n",
        "Plot Plan\n",
        "\n",
        "Xâ€‘axis: GDP per capita (ideally logâ€‘scaled)\n",
        "\n",
        "Yâ€‘axis: Life expectancy\n",
        "\n",
        "Optional enhancements: Color/size by population or continent\n",
        "\n",
        "Visualization Tools\n",
        "Using Python with libraries like pandas, matplotlib, and seaborn, we can:\n",
        "\n",
        "Load the CSV\n",
        "\n",
        "Filter (e.g., for a specific year)\n",
        "\n",
        "Produce a scatter plot â€” with optional log-scaling for clarity\n",
        "\n",
        "ðŸ”§ Example Code (Python)"
      ],
      "metadata": {
        "id": "vZf28VVhply0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (1950â€“2018)\n",
        "df = pd.read_csv('Life Expectancy vs GDP 1950-2018.csv')\n",
        "\n",
        "# Select a specific year (e.g., 2018)\n",
        "year = 2018\n",
        "data = df[df.year == year]\n",
        "\n",
        "# Create scatter plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=data, x='gdpPercap', y='lifeExp',\n",
        "                size='population', hue='continent', alpha=0.7, edgecolor='grey')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('GDP per Capita (log scale)')\n",
        "plt.ylabel('Life Expectancy (years)')\n",
        "plt.title(f'GDP per Capita vs Life Expectancy â€” {year}')\n",
        "plt.legend(title='Continent', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k_pRWrxrqxMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Implement and compare simple random sampling and systematic sampling.\n",
        "  - Simple Random Sampling (SRS)\n",
        "In SRS, every individual in the population has an equal chance of being selected. It's akin to drawing names out of a hat.\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "JndjSqCYq4nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample population dataset\n",
        "data = pd.DataFrame({\n",
        "    'ID': np.arange(1, 101),\n",
        "    'Name': ['Student_' + str(i) for i in range(1, 101)]\n",
        "})\n",
        "\n",
        "# Set sample size\n",
        "sample_size = 10\n",
        "\n",
        "# Perform simple random sampling\n",
        "simple_random_sample = data.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(\"Simple Random Sample:\")\n",
        "print(simple_random_sample)\n"
      ],
      "metadata": {
        "id": "83f-9o95rf8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "GqZy21Ekrkrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Simple Random Sample:\n",
        "    ID         Name\n",
        "2    3   Student_3\n",
        "5    6   Student_6\n",
        "8    9   Student_9\n",
        "...\n"
      ],
      "metadata": {
        "id": "Fahrl0bJrlk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Systematic Sampling\n",
        "In systematic sampling, you select every nth individual from an ordered list, starting from a random point.\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "iG5mjTSErqzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sample size and interval\n",
        "N = len(data)  # Population size\n",
        "n = sample_size  # Desired sample size\n",
        "k = N // n  # Sampling interval\n",
        "\n",
        "# Randomly choose a starting point\n",
        "np.random.seed(42)\n",
        "start = np.random.randint(0, k)\n",
        "\n",
        "# Select every k-th element\n",
        "systematic_sample = data.iloc[start::k]\n",
        "\n",
        "print(\"Systematic Sample:\")\n",
        "print(systematic_sample)\n"
      ],
      "metadata": {
        "id": "qH4Wm7ZorrtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "79PdF7ForvCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Systematic Sample:\n",
        "    ID         Name\n",
        "0    1   Student_1\n",
        "10   11  Student_11\n",
        "20   21  Student_21\n",
        "...\n"
      ],
      "metadata": {
        "id": "LcYRXgnBrw6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison\n",
        "Feature\tSimple Random Sampling\tSystematic Sampling\n",
        "Selection Method\tRandom selection\tFixed interval after random start\n",
        "Bias Risk\tLower (if population is homogeneous)\tHigher if there's an underlying pattern\n",
        "Efficiency\tMay require more resources\tMore efficient for large datasets\n",
        "Implementation\tStraightforward\tRequires careful interval calculation\n",
        "\n",
        "ðŸ“Š Visual Comparison\n",
        "To visualize the differences, let's plot the selected samples."
      ],
      "metadata": {
        "id": "o66Cg4R_r0Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Simple Random Sample\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(simple_random_sample['ID'], np.zeros_like(simple_random_sample['ID']), color='blue', label='SRS')\n",
        "plt.title('Simple Random Sample')\n",
        "plt.yticks([])\n",
        "\n",
        "# Systematic Sample\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(systematic_sample['ID'], np.zeros_like(systematic_sample['ID']), color='green', label='Systematic')\n",
        "plt.title('Systematic Sample')\n",
        "plt.yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CmxqiT8Pr7Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visualization will display two plots side by side, showing the distribution of samples for each method.\n",
        "\n",
        "âœ… Conclusion\n",
        "Simple Random Sampling is ideal when you want each individual to have an equal chance of selection, minimizing bias.\n",
        "\n",
        "Systematic Sampling is more efficient for large datasets but can introduce bias if there's an underlying pattern in the population.\n",
        "\n",
        "The choice between these methods depends on the specific requirements of your study and the nature of your population.\n",
        "\n"
      ],
      "metadata": {
        "id": "uEzAaVWisDmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Calculate the mean, median, and mode of grouped data.\n",
        " -  ðŸ”¸ 1. Mean (Estimated Average)\n",
        "Formula:\n",
        "ð‘¥\n",
        "Ë‰\n",
        "=\n",
        "âˆ‘\n",
        "ð‘“\n",
        "ð‘–\n",
        "ð‘¥\n",
        "ð‘–\n",
        "âˆ‘\n",
        "ð‘“\n",
        "ð‘–\n",
        "x\n",
        "Ë‰\n",
        " =\n",
        "âˆ‘f\n",
        "i\n",
        "â€‹\n",
        "\n",
        "âˆ‘f\n",
        "i\n",
        "â€‹\n",
        " x\n",
        "i\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "\n",
        "ð‘¥\n",
        "ð‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  = midpoint of each class\n",
        "\n",
        "ð‘“\n",
        "ð‘–\n",
        "f\n",
        "i\n",
        "â€‹\n",
        "  = frequency for that class\n",
        "\n",
        "Estimate using midpoints for intervals.\n",
        "reddit.com\n",
        "+15\n",
        "Example."
      ],
      "metadata": {
        "id": "6ync5tc5sJF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "| Class     | Midpoint $x_i$ | Frequency $f_i$ | $f_i x_i$ |\n",
        "| --------- | -------------- | --------------- | --------- |\n",
        "| 0â€“10      | 5              | 8               | 40        |\n",
        "| 10â€“20     | 15             | 16              | 240       |\n",
        "| 20â€“30     | 25             | 36              | 900       |\n",
        "| 30â€“40     | 35             | 34              | 1190      |\n",
        "| 40â€“50     | 45             | 6               | 270       |\n",
        "| **Total** |                | **100**         | **2640**  |\n"
      ],
      "metadata": {
        "id": "tQbOhDJDw5QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x\n",
        "Ë‰\n",
        " =2640/100=26.40"
      ],
      "metadata": {
        "id": "ca5ZScZKxfpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Median (Estimated Middle Value)\n",
        "Formula:\n",
        "Median\n",
        "=\n",
        "ð‘™\n",
        "+\n",
        "(\n",
        "ð‘\n",
        "2\n",
        "âˆ’\n",
        "ð‘\n",
        "ð‘“\n",
        "ð‘“\n",
        ")\n",
        "Ã—\n",
        "â„Ž\n",
        "Median=l+(\n",
        "f\n",
        "2\n",
        "N\n",
        "â€‹\n",
        " âˆ’c\n",
        "f\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " )Ã—h\n",
        "\n",
        "ð‘™\n",
        "l = lower boundary of median class\n",
        "\n",
        "ð‘\n",
        "N = total frequency\n",
        "\n",
        "ð‘\n",
        "ð‘“\n",
        "c\n",
        "f\n",
        "â€‹\n",
        "  = cumulative frequency before median class\n",
        "\n",
        "ð‘“\n",
        "f = frequency of median class\n",
        "\n",
        "â„Ž\n",
        "h = class width\n",
        "\n",
        "Use\n",
        "ð‘\n",
        "/\n",
        "2\n",
        "N/2 to locate the class containing the median.\n",
        "learning.box\n",
        "+9\n",
        "mathsisfun.com\n",
        "+9\n",
        "vrcacademy.com\n",
        "+9\n",
        "\n",
        "Example (same data):\n",
        "\n",
        "ð‘\n",
        "=\n",
        "100\n",
        "N=100, so\n",
        "ð‘\n",
        "/\n",
        "2\n",
        "=\n",
        "50\n",
        "N/2=50.\n",
        "\n",
        "Cumulative frequencies: 8, 24, 60 â†’ median class is 20â€“30.\n",
        "\n",
        "ð‘™\n",
        "=\n",
        "20\n",
        ",\n",
        "ð‘\n",
        "ð‘“\n",
        "=\n",
        "24\n",
        ",\n",
        "ð‘“\n",
        "=\n",
        "36\n",
        ",\n",
        "â„Ž\n",
        "=\n",
        "10\n",
        "l=20,c\n",
        "f\n",
        "â€‹\n",
        " =24,f=36,h=10.\n",
        "\n",
        "Median\n",
        "=\n",
        "20\n",
        "+\n",
        "(\n",
        "50\n",
        "âˆ’\n",
        "24\n",
        "36\n",
        ")\n",
        "Ã—\n",
        "10\n",
        "=\n",
        "27.22\n",
        "Median=20+(\n",
        "36\n",
        "50âˆ’24\n",
        "â€‹\n",
        " )Ã—10=27.22\n",
        "reddit.com\n",
        "geeksforgeeks.org\n",
        "+11\n",
        "geeksforgeeks.org\n",
        "+11\n",
        "geeksforgeeks.org\n",
        "+11\n",
        "\n",
        "ðŸ”¸ 3. Mode (Estimated Most Frequent Value)\n",
        "Formula:\n",
        "Mode\n",
        "=\n",
        "ð¿\n",
        "+\n",
        "(\n",
        "ð‘“\n",
        "ð‘š\n",
        "âˆ’\n",
        "ð‘“\n",
        "1\n",
        "2\n",
        "ð‘“\n",
        "ð‘š\n",
        "âˆ’\n",
        "ð‘“\n",
        "1\n",
        "âˆ’\n",
        "ð‘“\n",
        "2\n",
        ")\n",
        "Ã—\n",
        "â„Ž\n",
        "Mode=L+(\n",
        "2f\n",
        "m\n",
        "â€‹\n",
        " âˆ’f\n",
        "1\n",
        "â€‹\n",
        " âˆ’f\n",
        "2\n",
        "â€‹\n",
        "\n",
        "f\n",
        "m\n",
        "â€‹\n",
        " âˆ’f\n",
        "1\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " )Ã—h\n",
        "\n",
        "ð¿\n",
        "L = lower boundary of modal (most frequent) class\n",
        "\n",
        "ð‘“\n",
        "ð‘š\n",
        "f\n",
        "m\n",
        "â€‹\n",
        "  = frequency of modal class\n",
        "\n",
        "ð‘“\n",
        "1\n",
        "f\n",
        "1\n",
        "â€‹\n",
        " ,\n",
        "ð‘“\n",
        "2\n",
        "f\n",
        "2\n",
        "â€‹\n",
        "  = frequencies of preceding and succeeding classes\n",
        "\n",
        "â„Ž\n",
        "h = class width\n",
        "\n",
        "Identify the modal class (peak frequency) first, then apply formula.\n",
        "mathsisfun.com\n",
        "+5\n",
        "geeksforgeeks.org\n",
        "+5\n",
        "vrcacademy.com\n",
        "+5\n",
        "en.wikipedia.org\n",
        "+6\n",
        "cuemath.com\n",
        "+6\n",
        "vrcacademy.com\n",
        "+6\n",
        "\n",
        "Example (same data):\n",
        "\n",
        "Modal class is 40â€“50 with\n",
        "ð‘“\n",
        "ð‘š\n",
        "=\n",
        "28\n",
        "f\n",
        "m\n",
        "â€‹\n",
        " =28,\n",
        "ð‘“\n",
        "1\n",
        "=\n",
        "12\n",
        "f\n",
        "1\n",
        "â€‹\n",
        " =12,\n",
        "ð‘“\n",
        "2\n",
        "=\n",
        "20\n",
        "f\n",
        "2\n",
        "â€‹\n",
        " =20,\n",
        "ð¿\n",
        "=\n",
        "40\n",
        "L=40,\n",
        "â„Ž\n",
        "=\n",
        "10\n",
        "h=10.\n",
        "\n",
        "Mode\n",
        "=\n",
        "40\n",
        "+\n",
        "(\n",
        "28\n",
        "âˆ’\n",
        "12\n",
        "2\n",
        "â‹…\n",
        "28\n",
        "âˆ’\n",
        "12\n",
        "âˆ’\n",
        "20\n",
        ")\n",
        "Ã—\n",
        "10\n",
        "=\n",
        "46.67\n",
        "Mode=40+(\n",
        "2â‹…28âˆ’12âˆ’20\n",
        "28âˆ’12\n",
        "â€‹\n",
        " )Ã—10=46.67\n",
        "geeksforgeeks.org\n",
        "\n",
        "Alternatively, you can use the empirical relation (only if suited):\n",
        "Mode\n",
        "â‰ˆ\n",
        "3\n",
        "Ã—\n",
        "Median\n",
        "âˆ’\n",
        "2\n",
        "Ã—\n",
        "Mean\n",
        "Modeâ‰ˆ3Ã—Medianâˆ’2Ã—Mean\n",
        "mathsisfun.com\n",
        "+9\n",
        "geeksforgeeks.org\n",
        "+9\n",
        "byjus.com\n",
        "+9\n",
        "\n",
        "âœ… Summary Table\n",
        "Measure\tPurpose\tFormula Summary\n",
        "Mean\tCentral__estimate__using midpoints\n",
        "âˆ‘\n",
        "ð‘“\n",
        "ð‘–\n",
        "ð‘¥\n",
        "ð‘–\n",
        "/\n",
        "âˆ‘\n",
        "ð‘“\n",
        "ð‘–\n",
        "âˆ‘f\n",
        "i\n",
        "â€‹\n",
        " x\n",
        "i\n",
        "â€‹\n",
        " /âˆ‘f\n",
        "i\n",
        "â€‹\n",
        "\n",
        "Median\tMiddle__value estimation\n",
        "ð‘™\n",
        "+\n",
        "(\n",
        "(\n",
        "ð‘\n",
        "/\n",
        "2\n",
        "âˆ’\n",
        "ð‘\n",
        "ð‘“\n",
        ")\n",
        "/\n",
        "ð‘“\n",
        ")\n",
        "Ã—\n",
        "â„Ž\n",
        "l+((N/2âˆ’c\n",
        "f\n",
        "â€‹\n",
        " )/f)Ã—h\n",
        "Mode\tMost__frequent__estimate\n",
        "ð¿\n",
        "+\n",
        "ð‘“\n",
        "ð‘š\n",
        "âˆ’\n",
        "ð‘“\n",
        "1\n",
        "2\n",
        "ð‘“\n",
        "ð‘š\n",
        "âˆ’\n",
        "ð‘“\n",
        "1\n",
        "âˆ’\n",
        "ð‘“\n",
        "2\n",
        "Ã—\n",
        "â„Ž\n",
        "L+\n",
        "2f\n",
        "m\n",
        "â€‹\n",
        " âˆ’f\n",
        "1\n",
        "â€‹\n",
        " âˆ’f\n",
        "2\n",
        "â€‹\n",
        "\n",
        "f\n",
        "m\n",
        "â€‹\n",
        " âˆ’f\n",
        "1\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " Ã—h\n",
        "\n"
      ],
      "metadata": {
        "id": "OvBv5jBYxhry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Simulate data using Python and calculate its central tendency and dispersion.\n",
        " - Hereâ€™s a clear and practical example of simulating data in Python to compute its central tendency (mean, median, mode) and dispersion (variance, standard deviation, range):\n",
        "\n",
        "ðŸ Python Code Example"
      ],
      "metadata": {
        "id": "aOAC0LDixydJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import statistics as st\n",
        "from scipy import stats\n",
        "\n",
        "# 1. Generate random data (normal distribution)\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# 2. Central tendency\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "try:\n",
        "    mode = st.mode(data)\n",
        "except st.StatisticsError:\n",
        "    mode = stats.mode(data, keepdims=False).mode  # fallback if no unique mode\n",
        "\n",
        "# 3. Dispersion\n",
        "variance = np.var(data, ddof=0)         # population variance\n",
        "sample_variance = np.var(data, ddof=1)  # sample variance\n",
        "std = np.std(data, ddof=0)              # population standard deviation\n",
        "sample_std = np.std(data, ddof=1)       # sample standard deviation\n",
        "data_range = np.ptp(data)               # max â€“ min\n",
        "\n",
        "# 4. Summary print-out\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n",
        "print(f\"Variance (pop): {variance:.2f}\")\n",
        "print(f\"Std Dev (pop): {std:.2f}\")\n",
        "print(f\"Sample Variance: {sample_variance:.2f}\")\n",
        "print(f\"Sample Std Dev: {sample_std:.2f}\")\n",
        "print(f\"Range: {data_range:.2f}\")\n"
      ],
      "metadata": {
        "id": "ynNOW5Ubyc-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“š Explanation & Notes\n",
        "We use np.random.normal(...) to generate 1,000 simulated values with a true mean of 50 and standard deviation of 10\n",
        "docs.python.org\n",
        "+2\n",
        "medium.com\n",
        "+2\n",
        "reddit.com\n",
        "+2\n",
        "medium.com\n",
        "reneshbedre.com\n",
        "reddit.com\n",
        "+12\n",
        "realpython.com\n",
        "+12\n",
        "docs.python.org\n",
        "+12\n",
        ".\n",
        "\n",
        "numpy computes mean, median, std dev, variance, and range efficiently .\n",
        "\n",
        "Mode can be tricky for continuous dataâ€”Pythonâ€™s statistics.mode may raise an error if there's no clear single mode, so we use scipy.stats.mode(...) as a fallback.\n",
        "âœ… Expected Output Example"
      ],
      "metadata": {
        "id": "srK2nIByyftu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mean: 49.84\n",
        "Median: 49.82\n",
        "Mode: 26.45     # may vary wildly for continuous data\n",
        "Variance (pop): 98.50\n",
        "Std Dev (pop): 9.92\n",
        "Sample Variance: 99.49\n",
        "Sample Std Dev: 9.97\n",
        "Range: 61.23\n"
      ],
      "metadata": {
        "id": "QdvFMOyYyjIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean and median should be close to 50, as generated.\n",
        "\n",
        "The mode for continuous normal data is unstableâ€”it might pick any single point.\n",
        "\n",
        "The variance and standard deviation align with the chosen scale=10.\n",
        "\n",
        "data_range gives a quick sense of spread."
      ],
      "metadata": {
        "id": "J68t2q2rysjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Use NumPy or pandas to summarize a datasetâ€™s descriptive statistics.\n",
        " - Hereâ€™s a polished and informative example using pandas (with NumPy where needed) to summarize a datasetâ€™s descriptive statistics, including central tendency and dispersion:\n",
        "\n",
        "ðŸ“„ 1. Load & Inspect the Data"
      ],
      "metadata": {
        "id": "7MLEhKz9ytfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For demo, load Iris dataset via seaborn or any CSV\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "K9MMAUYOzFeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives you:\n",
        "\n",
        ".shape â†’ number of rows Ã— columns\n",
        "\n",
        ".info() â†’ data types and non-null counts\n",
        "\n",
        ".head() â†’ first few records"
      ],
      "metadata": {
        "id": "LFyFCJ_6zUR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Built-in Summary with describe()"
      ],
      "metadata": {
        "id": "FpUB_DnAzYS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_summary = df.describe()\n",
        "print(numeric_summary)\n"
      ],
      "metadata": {
        "id": "KOlHJdxWzZeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output includes count, mean, std, min, 25%, 50%, 75%, max for each numeric column.\n",
        "For mixed columns (like species strings), include:"
      ],
      "metadata": {
        "id": "o_uk4BDizbpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_summary = df.describe(include='all')\n",
        "print(full_summary)\n"
      ],
      "metadata": {
        "id": "f8WPv0vzzf8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual Measures\n",
        "You can compute specific statistics on a per-column basis:"
      ],
      "metadata": {
        "id": "K5nYE9g3zh7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = df['sepal_length']\n",
        "col_mean = col.mean()\n",
        "col_median = col.median()\n",
        "col_std = col.std()\n",
        "col_var = col.var()\n",
        "col_min = col.min()\n",
        "col_max = col.max()\n",
        "col_quantiles = col.quantile([0.25, 0.75])\n"
      ],
      "metadata": {
        "id": "l8iI6srszmab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mean(), median(), std(), var(), min(), max(), quantile()\n",
        "\n",
        "Or with NumPy:"
      ],
      "metadata": {
        "id": "nywhxdYJzrqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(col), np.median(col), np.std(col), np.var(col)\n"
      ],
      "metadata": {
        "id": "-fgMuGzyzscO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Aggregation with .agg()\n",
        "Get multiple statistics in one go:"
      ],
      "metadata": {
        "id": "8A4EGspAzugs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = df['sepal_length'].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
        "# Or for multiple columns:\n",
        "stats_multi = df.agg({\n",
        "    'sepal_length': ['mean', 'std', 'min', 'max'],\n",
        "    'petal_length': ['mean', 'median', 'var']\n",
        "})\n",
        "print(stats_multi)\n"
      ],
      "metadata": {
        "id": "9BxfEUyXzxCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Metrics: Range, IQR, Variance\n",
        "You can complement with derived statistics:"
      ],
      "metadata": {
        "id": "Pj61xsYSzzHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sepal = df['sepal_length']\n",
        "data_range = sepal.max() - sepal.min()\n",
        "iqr = sepal.quantile(0.75) - sepal.quantile(0.25)\n",
        "variance = sepal.var()\n"
      ],
      "metadata": {
        "id": "6LaLjI8gz2V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Summary Table\n",
        "Task\tCode Example\n",
        "Quick overview\tdf.describe()\n",
        "Include categorical columns\tdf.describe(include='all')\n",
        "Single column stats\tdf['col'].mean(), .std(), etc.\n",
        "Multiple stats with .agg()\tdf.agg({...})\n",
        "Range, IQR, variance\tDerived manually using .max(), .min(), .quantile()\n",
        "\n",
        "ðŸ§· Complete Minimal Example"
      ],
      "metadata": {
        "id": "224CFmMaz9Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "df = sns.load_dataset('iris')\n",
        "print(df.shape, df.info())\n",
        "\n",
        "# Summary stats\n",
        "print(df.describe())\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "# Custom column stats\n",
        "col = df['sepal_length']\n",
        "print('Mean:', col.mean(),\n",
        "      'Median:', col.median(),\n",
        "      'Std:', col.std(),\n",
        "      'Var:', col.var(),\n",
        "      'Range:', col.max()-col.min(),\n",
        "      'IQR:', col.quantile(0.75)-col.quantile(0.25))\n",
        "\n",
        "# Aggregated stats\n",
        "print(df.agg({\n",
        "    'sepal_length': ['mean', 'std', 'min', 'max'],\n",
        "    'petal_length': ['mean', 'median', 'var']\n",
        "}))\n"
      ],
      "metadata": {
        "id": "gJ8kSlv70CfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Takeaway\n",
        "Use df.describe() for a fast, comprehensive overview.\n",
        "\n",
        "Use .agg() to tailor the output.\n",
        "\n",
        "Use direct methods and NumPy for precise metrics like range, IQR, variance."
      ],
      "metadata": {
        "id": "bmXPVUZx0Gwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Plot a boxplot to understand the spread and identify outliers.\n",
        " - To visualize the spread of your data and spot outliers, a boxplot is an excellent tool. Here's how you can create one using pandas, Matplotlib, or Seaborn, and understand what it reveals:\n",
        "\n",
        "ðŸ“Š 1. Using pandas + Matplotlib"
      ],
      "metadata": {
        "id": "aYh1VcQS0Hll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "df = pd.DataFrame({\n",
        "    'A': np.random.normal(50, 10, 200),\n",
        "    'B': np.random.normal(60, 15, 200)\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "df.boxplot()\n",
        "plt.title('Boxplot of A and B')\n",
        "plt.ylabel('Values')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FT7W_Qj60X13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This draws side-by-side boxplots for columns A and B using pandasâ€™ .boxplot() built on Matplotlib\n",
        ".\n",
        "\n",
        "ðŸŽ¨ 2. Using Seaborn"
      ],
      "metadata": {
        "id": "6ay-S3uR0dKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=df, palette=\"Set2\", fliersize=4)\n",
        "plt.title('Seaborn Boxplot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "But_YJLT0hex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seaborn uses a whisker rule of whis=1.5, meaning whiskers extend to values within 1.5Ã—IQR, and anything beyond is flagged as an outlier ."
      ],
      "metadata": {
        "id": "xXf_HwHh0liC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Interpret the Plot\n",
        "Box: spans from Q1 (25th percentile) to Q3 (75th percentile)\n",
        "\n",
        "Line inside box: median (50th percentile)\n",
        "\n",
        "Whiskers: extend to the most extreme values that are within 1.5 Ã— IQR of the quartiles\n",
        "\n",
        "Points beyond whiskers: flagged as outliers\n",
        "towardsdatascience.com\n",
        "adventuresinmachinelearning.com\n",
        "\n",
        "Any value < Q1 âˆ’ 1.5 Ã— IQR or > Q3 + 1.5 Ã— IQR is marked as an outlier\n",
        "reddit.com\n",
        "+10\n",
        "askpython.com\n"
      ],
      "metadata": {
        "id": "nX9-0kJs0qHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the Appearance\n",
        "Hide outliers:"
      ],
      "metadata": {
        "id": "ixvcUbZj0wBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df, showfliers=False)\n"
      ],
      "metadata": {
        "id": "L9Zh_Wn30wxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change outlier marker size:"
      ],
      "metadata": {
        "id": "6uiQh4de00Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df, fliersize=3)\n",
        "``` :contentReference[oaicite:11]{index=11}\n"
      ],
      "metadata": {
        "id": "438-3Y-G07H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Summary\n",
        "A boxplot quickly communicates:\n",
        "\n",
        "âž• Central tendency (via median)\n",
        "\n",
        "ðŸŒ Spread (via IQR and whiskers)\n",
        "\n",
        "ðŸš© Outliers (points beyond whiskers)\n",
        "\n",
        "Itâ€™s especially useful for comparing variables or groups in your dataset."
      ],
      "metadata": {
        "id": "pQLqw3Qx0_q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Calculate the interquartile range (IQR) of a dataset.\n",
        " - What is the IQR?\n",
        "IQR = Qâ‚ƒ â€“ Qâ‚, where Qâ‚ is the 25th percentile (first quartile) and Qâ‚ƒ is the 75th percentile (third quartile)\n",
        "docs.scipy.org\n",
        "+15\n",
        "en.wikipedia.org\n",
        "+15\n",
        "sqlpey.com\n",
        "+15\n",
        ".\n",
        "\n",
        "It measures the spread of the middle 50% of your dataâ€”a robust indicator of dispersion, less sensitive to outliers"
      ],
      "metadata": {
        "id": "5GV3lhc91A_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… How to get IQR in Python\n",
        "ðŸ§® Option 1: Using NumPy"
      ],
      "metadata": {
        "id": "TWbUwiHx1Vel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([...])  # your numeric data\n",
        "\n",
        "# Using percentile directly\n",
        "q75, q25 = np.percentile(data, [75, 25])\n",
        "iqr = q75 - q25\n",
        "\n",
        "print(\"IQR:\", iqr)\n"
      ],
      "metadata": {
        "id": "hXlPvwK91YNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: Using SciPy"
      ],
      "metadata": {
        "id": "W55TDi_J1cW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import iqr\n",
        "\n",
        "iqr_value = iqr(data)  # by default computes Q3 âˆ’ Q1\n",
        "print(\"IQR (SciPy):\", iqr_value)\n"
      ],
      "metadata": {
        "id": "_SK9PwCE1dCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Quick Example"
      ],
      "metadata": {
        "id": "B75P87FE1k_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import iqr\n",
        "\n",
        "data = np.array([4.1, 6.2, 6.7, 7.1, 7.4, 7.4, 7.9, 8.1])\n",
        "\n",
        "# Method A: NumPy\n",
        "q75, q25 = np.percentile(data, [75, 25])\n",
        "print(\"NumPy IQR:\", q75 - q25)\n",
        "\n",
        "# Method B: SciPy\n",
        "print(\"SciPy IQR:\", iqr(data))\n",
        "\n",
        "# Using midpoint interpolation\n",
        "print(\"SciPy IQR (midpoint):\", iqr(data, interpolation='midpoint'))\n"
      ],
      "metadata": {
        "id": "NszhZ7_f1lp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ Summary"
      ],
      "metadata": {
        "id": "vGEUWcfa1qTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "| Task                     | Code Snippet                                  |\n",
        "| ------------------------ | --------------------------------------------- |\n",
        "| **NumPy** IQR (%)        | `np.percentile(data, [75,25])` â†’ `q75 - q25`  |\n",
        "| **Efficient NumPy**      | `np.subtract(*np.percentile(data, [75, 25]))` |\n",
        "| **SciPy** IQR            | `iqr(data)`                                   |\n",
        "| **Consistent endpoints** | `iqr(data, interpolation='midpoint')`         |\n",
        "| **Manual formula**       | `IQR = Q3 â€“ Q1` (quartile definition based)   |\n"
      ],
      "metadata": {
        "id": "i0GP80271q5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”š Key Takeaway\n",
        "The IQR is a robust way to understand spread without being skewed by extremes. In Python, choose NumPy for simplicity or SciPy for built-in convenienceâ€”and be mindful of quartile definitions if your dataset is small or precision-critical.\n",
        "\n",
        "Need help applying this to your own dataset or comparing it with other dispersion measures? Just let me know!"
      ],
      "metadata": {
        "id": "tzHEboA41vPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Implement Z-score normalization and explain its significance.\n",
        " - Hereâ€™s how to implement Zâ€‘score normalization in Python and why itâ€™s a crucial technique:\n",
        "\n",
        "ðŸ§® 1. Code Example (NumPy & pandas)"
      ],
      "metadata": {
        "id": "lHeC4lHR1wOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data array\n",
        "data = np.array([70, 80, 90, 100, 110])\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "mean = data.mean()\n",
        "std = data.std(ddof=0)  # population std\n",
        "\n",
        "# Z-score normalization using NumPy\n",
        "z_scores_np = (data - mean) / std\n",
        "print(\"NumPy Z-scores:\", z_scores_np)\n",
        "\n",
        "# If using pandas DataFrame\n",
        "df = pd.DataFrame({'score': data})\n",
        "df['z_score'] = (df['score'] - df['score'].mean()) / df['score'].std(ddof=0)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "L8UrK0nJ17EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ” Expected output:"
      ],
      "metadata": {
        "id": "1Xg-rbaI2Dtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NumPy Z-scores: [-1.4142, -0.7071, 0, 0.7071, 1.4142]\n"
      ],
      "metadata": {
        "id": "N29heGbC2EhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This transforms raw values to units of standard deviation from the mean"
      ],
      "metadata": {
        "id": "-3BlIYhg2JDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare two datasets using their standard deviations.\n",
        " - When comparing two datasets using standard deviations (SDs), you're essentially comparing their spread or consistency around their respective means. Here's a structured guide:\n",
        "\n",
        "ðŸ“ˆ 1. What Standard Deviation Reveals\n",
        "Low SD: data points cluster closely around the mean â€” more consistent\n",
        "\n",
        "High SD: data are more spread out â€” less consistent\n",
        "investopedia.com\n",
        "+15\n",
        "en.wikipedia.org\n",
        "+15\n",
        "shiksha.com\n",
        "+15\n",
        "\n",
        "ðŸ” 2. Direct Comparison\n",
        "If two datasets have similar means:\n",
        "\n",
        "Compare SDs directly:\n",
        "\n",
        "Largerâ€¯SD â†’ greater variability\n",
        "\n",
        "Smallerâ€¯SD â†’ more uniform\n",
        "Example: Pant's cricket scores SD=36.96 vs Kartik's 17.91 â†’ Pant's is less consistent\n",
        "shiksha.com\n",
        "+1\n",
        "vaia.com\n",
        "+1\n",
        "investopedia.com\n",
        "+1\n",
        "reddit.com\n",
        "+1\n",
        "en.wikipedia.org\n",
        "+2\n",
        "investopedia.com\n",
        "+2\n",
        "en.wikipedia.org\n",
        "+2\n",
        "reddit.com\n",
        "+6\n",
        "vaia.com\n",
        "+6\n",
        "statisticsbyjim.com\n",
        "+6\n",
        "\n",
        "If means differ widely:\n",
        "\n",
        "SD alone may mislead due to different scales\n",
        "reddit.com\n",
        "+2\n",
        "pages.uoregon.edu\n",
        "+2\n",
        "stats.libretexts.org\n",
        "+2\n",
        "\n",
        "ðŸ“ 3. Use Coefficient of Variation (CV)\n",
        "CV = SDâ€¯/â€¯mean\n",
        "A unitless ratio that contextualizes variability across datasets with different means or units\n",
        "vaia.com\n",
        "+15\n",
        "en.wikipedia.org\n",
        "+15\n",
        "statisticsbyjim.com\n",
        "+15\n",
        "\n",
        "E.g. two datasets, same mean (25) but SDs of 4.5 vs 6.7 â†’ CVs 18% vs 26.8% â†’ the latter is more variable\n",
        "investopedia.com\n",
        "+3\n",
        "onlinemath4all.com\n",
        "+3\n",
        "reddit.com\n",
        "+3\n",
        "\n",
        "ðŸ§® 4. Statistical Testing: Are SDs Different?\n",
        "To test if the variability differs significantly:\n",
        "\n",
        "F-test compares variances (SDÂ²); p<0.05 indicates real difference\n",
        "discuss.codecademy.com\n",
        "+4\n",
        "medcalc.org\n",
        "+4\n",
        "reddit.com\n",
        "+4\n",
        "reddit.com\n",
        "+2\n",
        "graphstats.net\n",
        "+2\n",
        "graphpad.com\n",
        "+2\n",
        "\n",
        "Leveneâ€™s test is more robust against non-normal distributions\n",
        "investopedia.com\n",
        "+15\n",
        "reddit.com\n",
        "+15\n",
        "medcalc.org\n",
        "+15\n",
        "\n",
        "ðŸ§ª 5. Practical Example and Interpretation\n",
        "Dataset A: mean =â€¯110, SD =â€¯15\n",
        "Dataset B: mean =â€¯107, SD =â€¯14\n",
        "\n",
        "SDs are similar â†’ variability nearly equal\n",
        "\n",
        "Compute effect size (Cohenâ€™s d â‰ˆ (110â€“107)/pooled SD â‰ˆ 0.21), indicating a small difference\n",
        "stats.libretexts.org\n",
        "+4\n",
        "pages.uoregon.edu\n",
        "+4\n",
        "onlinemath4all.com\n",
        "+4\n",
        "\n",
        "âœ… Summary\n",
        "Scenario\tRecommended Comparison\n",
        "Similar means\tCompare SDs directly\n",
        "Different means/units\tUse Coefficient of Variation\n",
        "Need statistical inference\tPerform F-test or Leveneâ€™s test\n",
        "Compare consistency\tLower SD/CV â†’ more consistent"
      ],
      "metadata": {
        "id": "Kkm54Ydd2Msj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§· Python Example"
      ],
      "metadata": {
        "id": "JSpJ5FT12kn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway, levene\n",
        "\n",
        "a = np.array([...])\n",
        "b = np.array([...])\n",
        "\n",
        "sd_a, sd_b = a.std(ddof=1), b.std(ddof=1)\n",
        "cv_a, cv_b = sd_a/a.mean(), sd_b/b.mean()\n",
        "\n",
        "# F-test\n",
        "f_stat = (a.var(ddof=1)/b.var(ddof=1))\n",
        "# or use Levene\n",
        "stat, p = levene(a, b)\n",
        "\n",
        "print(f\"SDs: {sd_a:.2f}, {sd_b:.2f}\")\n",
        "print(f\"CVs: {cv_a:.2%}, {cv_b:.2%}\")\n",
        "print(f\"Levene p-value: {p:.4f}\")\n"
      ],
      "metadata": {
        "id": "stjSynTd2leO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ Key Takeaway\n",
        "Standard deviation shows absolute spread.\n",
        "\n",
        "Coefficient of variation captures relative spread.\n",
        "\n",
        "Statistical tests tell you if differences in spread are unlikely due to chance."
      ],
      "metadata": {
        "id": "EIEbGxTt2zF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2o.  Write a Python program to visualize covariance using a heatmap.\n",
        " - Hereâ€™s a step-by-step Python example that computes and visualizes the covariance matrix of a dataset using pandas and Seaborn:\n",
        "\n",
        "ðŸ Step-by-Step Code"
      ],
      "metadata": {
        "id": "uCrnoKwK2zy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create example DataFrame (or load your own)\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'Feature1': np.random.normal(0, 1, 100),\n",
        "    'Feature2': np.random.normal(5, 2, 100),\n",
        "    'Feature3': np.random.normal(-2, 5, 100),\n",
        "})\n",
        "\n",
        "# 2. Compute covariance matrix\n",
        "cov_matrix = df.cov()\n",
        "print(cov_matrix)\n",
        "\n",
        "# 3. Plot covariance heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cov_matrix, cmap='Blues', annot=True, fmt=\".2f\", square=True)\n",
        "plt.title('Covariance Matrix Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BAiQJQCY29ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why It Works\n",
        ".cov() from pandas computes pairwise covariances among numeric features\n",
        "towardsdatascience.com\n",
        "+15\n",
        "adventuresinmachinelearning.com\n",
        "+15\n",
        "towardsai.net\n",
        "+15\n",
        "alpharithms.com\n",
        "reddit.com\n",
        ".\n",
        "\n",
        "Seabornâ€™s heatmap() visualizes the matrix, using color intensity to reveal covariance magnitude.\n",
        "\n",
        "annot=True labels cells with covariance values.\n",
        "\n",
        "cmap='Blues' offers a clear, intuitive color map ."
      ],
      "metadata": {
        "id": "96OLwEAo3DM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Use seaborn to create a correlation matrix for a dataset.\n",
        " - Hereâ€™s a polished guide on how to create and customize a correlation matrix heatmap using Seaborn in Python:\n",
        "\n",
        "ðŸ Step-by-Step Code Example"
      ],
      "metadata": {
        "id": "UjsGVGAe3LBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset (e.g., iris)\n",
        "df = sns.load_dataset('iris')\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# 2. Compute correlation matrix\n",
        "corr = numeric_df.corr()\n",
        "\n",
        "# 3. Plot heatmap with full matrix and annotations\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
        "            annot=True, fmt=\".2f\", square=True, linewidths=0.5)\n",
        "plt.title('Correlation Matrix (Iris)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "llHbOuu53SNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why It Matters\n",
        "A correlation heatmap reveals relationships between variables at a glance.\n",
        "\n",
        "Positive values (blue/red, depending on palette) indicate increasing together, negative indicate inverse relationships.\n",
        "\n",
        "It helps detect multicollinearity, identify key predictors, and guide feature selection\n",
        "comet.com\n",
        "+15"
      ],
      "metadata": {
        "id": "Zy6pRrP53foc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Generate a dataset and implement both variance and standard deviation computations.\n",
        " - Hereâ€™s a complete Python example that generates a dataset, then computes both variance and standard deviation using NumPy via built-in functions and manually, ensuring clarity and reproducibility:\n",
        "\n"
      ],
      "metadata": {
        "id": "BIQXm4Ve3glj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Example"
      ],
      "metadata": {
        "id": "oanpfvYs4jJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Generate a random dataset (normal distribution)\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=10, scale=2, size=500)\n",
        "\n",
        "# 2. Built-in NumPy computations\n",
        "var_builtin = np.var(data)         # population variance (ddof=0)\n",
        "std_builtin = np.std(data)         # population standard deviation\n",
        "\n",
        "# 3. Manual calculations using formulas\n",
        "mean = np.mean(data)\n",
        "deviations = (data - mean) ** 2\n",
        "var_manual = deviations.mean()\n",
        "std_manual = np.sqrt(var_manual)\n",
        "\n",
        "# 4. Display results\n",
        "print(f\"Mean: {mean:.3f}\")\n",
        "print(f\"Variance (NumPy): {var_builtin:.3f}\")\n",
        "print(f\"Variance (manual): {var_manual:.3f}\")\n",
        "print(f\"Std Dev (NumPy): {std_builtin:.3f}\")\n",
        "print(f\"Std Dev (manual): {std_manual:.3f}\")\n"
      ],
      "metadata": {
        "id": "ndRvWpSd4j1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How It Works\n",
        "Data generation: np.random.normal(...) creates 500 samples from a normal distribution centered at 10 with an SD of 2.\n",
        "\n",
        "Built-in functions: np.var() and np.std() compute population variance and standard deviation directly\n",
        "reddit.com\n",
        "+15\n",
        "app.studyraid.com\n",
        "+15\n",
        "codefinity.com\n",
        "+15\n",
        "reddit.com\n",
        "geeksforgeeks.org\n",
        "+2\n",
        "reddit.com\n",
        "+2\n",
        "w3resource.com\n",
        "+2\n",
        "ihoctot.com\n",
        "+1\n",
        "reddit.com\n",
        "+1\n",
        "w3resource.com\n",
        "+3\n",
        "sparkcodehub.com\n",
        "+3\n",
        "codefinity.com\n",
        "+3\n",
        ".\n",
        "\n",
        "Manual method:\n",
        "\n",
        "Calculate the mean (mean).\n",
        "\n",
        "Compute squared deviations (x - mean)Â².\n",
        "\n",
        "Average these to get variance.\n",
        "\n",
        "Take square root for standard deviation\n",
        "ihoctot.com\n",
        "+9\n"
      ],
      "metadata": {
        "id": "1fAm6DB44rSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn.\n",
        " - ðŸ“Š Visualizing Skewness and Kurtosis\n",
        "1. Skewness: Measures the asymmetry of the distribution.\n",
        "Positive skew: Right tail is longer or fatter.\n",
        "\n",
        "Negative skew: Left tail is longer or fatter.\n",
        "\n",
        "2. Kurtosis: Measures the \"tailedness\" or sharpness of the peak.\n",
        "Leptokurtic: High kurtosis (>3), indicating heavy tails and a sharp peak.\n",
        "\n",
        "Platykurtic: Low kurtosis (<3), indicating light tails and a flat peak.\n",
        "\n",
        "Mesokurtic: Kurtosis â‰ˆ 3, similar to a normal distribution.\n",
        "\n",
        "ðŸ§ª Python Implementation"
      ],
      "metadata": {
        "id": "9QH-4wDh4sW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=10, scale=2, size=500)\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "data_skewness = skew(data)\n",
        "data_kurtosis = kurtosis(data)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data, kde=True, color='skyblue', stat='density', linewidth=0)\n",
        "plt.title(f'Skewness: {data_skewness:.2f}, Kurtosis: {data_kurtosis:.2f}', fontsize=14)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2CWbVux5qUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of Results\n",
        "Skewness:\n",
        "\n",
        "0: Perfectly symmetrical distribution.\n",
        "\n",
        "Positive: Right-skewed (longer right tail).\n",
        "\n",
        "Negative: Left-skewed (longer left tail).\n",
        "\n",
        "Kurtosis:\n",
        "\n",
        "3: Normal distribution.\n",
        "\n",
        ">3: Leptokurtic (heavy tails).\n",
        "\n",
        "<3: Platykurtic (light tails).\n",
        "\n"
      ],
      "metadata": {
        "id": "GvFzisn65s0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Implement the Pearson and Spearman correlation coefficients for a dataset.\n",
        " - ðŸ“Š Pearson Correlation Coefficient\n",
        "The Pearson correlation coefficient measures the linear relationship between two continuous variables. It assumes that the data is normally distributed and is sensitive to outliers.\n",
        "\n",
        "Code Example:"
      ],
      "metadata": {
        "id": "pKKQgsVh58z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Sample data\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "# Compute Pearson correlation\n",
        "corr, p_value = pearsonr(x, y)\n",
        "print(f\"Pearson correlation: {corr:.3f}, p-value: {p_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "HoTaw_7y6hxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "jOppBskt6kg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Pearson correlation: -1.000, p-value: 0.000\n"
      ],
      "metadata": {
        "id": "Iq8ZXwxe6m0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Pearson correlation of -1 indicates a perfect negative linear relationship between x and y.\n",
        "\n",
        "ðŸ“ˆ Spearman Rank Correlation Coefficient\n",
        "The Spearman rank correlation coefficient assesses how well the relationship between two variables can be described using a monotonic function. Unlike Pearson, Spearman does not assume a normal distribution and is less sensitive to outliers.\n",
        "\n",
        "Code Example:"
      ],
      "metadata": {
        "id": "NodnOZ-26o5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Compute Spearman correlation\n",
        "corr, p_value = spearmanr(x, y)\n",
        "print(f\"Spearman correlation: {corr:.3f}, p-value: {p_value:.3f}\")\n"
      ],
      "metadata": {
        "id": "TY0TaTeP6uMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "ZO9WwU4g6xiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Spearman correlation: -1.000, p-value: 0.000\n"
      ],
      "metadata": {
        "id": "Z8bBzrxQ6yg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Spearman correlation of -1 indicates a perfect negative monotonic relationship between x and y.\n",
        "\n",
        "ðŸ” Interpretation\n",
        "Coefficient\tRange\tInterpretation\n",
        "Pearson\t-1 to 1\tMeasures linear relationship\n",
        "Spearman\t-1 to 1\tMeasures monotonic relationship\n",
        "\n",
        "Pearson is suitable when the data is continuous, normally distributed, and the relationship is linear.\n",
        "\n",
        "Spearman is appropriate for ordinal data or when the relationship is monotonic but not necessarily linear.\n",
        "\n"
      ],
      "metadata": {
        "id": "wnv6sLaA60VL"
      }
    }
  ]
}